{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Ci9xQ2ME3ZJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/macbook/Downloads/testing (5)\n",
            "Execution time is 18.737971097998525 seconds\n"
          ]
        }
      ],
      "source": [
        "# this is the download function, it doenload only file that is less than 1 gig and also return the part of the file \n",
        "\n",
        "import os,requests\n",
        "import urllib\n",
        "import timeit\n",
        "def download(url):\n",
        "    max_size = 1000000000\n",
        "    req = urllib.request.Request(url, method='HEAD')\n",
        "    feed = urllib.request.urlopen(req)\n",
        "    size = int(feed.headers['Content-Length'])\n",
        "    if size < max_size:\n",
        "        audio_extenions = ['wav', 'mp3']\n",
        "        get_response = requests.get(url,stream=True)\n",
        "        file_name  = url.split(\"/\")[-1]\n",
        "        extension = file_name\n",
        "        extenstion = file_name.split(\"/\")[-1].split(\".\")[-1]\n",
        "        if extenstion in audio_extenions:\n",
        "            get_response = requests.get(url,stream=True)\n",
        "            file_name  = url.split(\"/\")[-1]\n",
        "            with open(file_name, 'wb') as f:\n",
        "                for chunk in get_response.iter_content(chunk_size=1024):\n",
        "                    if chunk: # filter out keep-alive new chunks\n",
        "                        f.write(chunk)\n",
        "                cwd = os.getcwd()\n",
        "                print(cwd)\n",
        "\n",
        "        else:\n",
        "            print(\"file not supported\")\n",
        "    else:\n",
        "        print(\"over size file\")\n",
        "    \n",
        "n = 1\n",
        "result = timeit.timeit(stmt='download(\"https://relen.s3.us-east-2.amazonaws.com/audio/samples/sample2.wav\")', globals=globals(), number=n)\n",
        "print(f\"Execution time is {result / n} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VklhYQIsNU2-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Import packages\n",
        "from typing import List, Dict\n",
        "import tqdm.notebook as tq\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast as T5Tokenizer\n",
        "    )\n",
        "\n",
        "MODEL_NAME = 't5-small'\n",
        "SOURCE_MAX_TOKEN_LEN = 300\n",
        "TARGET_MAX_TOKEN_LEN = 80\n",
        "SEP_TOKEN = '<sep>'\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.add_tokens(SEP_TOKEN)\n",
        "TOKENIZER_LEN = len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ-T5OmwNOv5"
      },
      "outputs": [],
      "source": [
        "class QGModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
        "        self.model.resize_token_embeddings(TOKENIZER_LEN) #resizing after adding new tokens to the tokenizer\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "  \n",
        "    def configure_optimizers(self):\n",
        "        return AdamW(self.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5MgWA6CNMPM"
      },
      "outputs": [],
      "source": [
        "def generate2(qgmodel: QGModel, context: str) -> str:\n",
        "    source_encoding = tokenizer(\n",
        "        ' {} {}'.format(SEP_TOKEN, context),\n",
        "        max_length=SOURCE_MAX_TOKEN_LEN,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    generated_ids = qgmodel.model.generate(\n",
        "        input_ids=source_encoding['input_ids'],\n",
        "        attention_mask=source_encoding['attention_mask'],\n",
        "        num_beams=1,\n",
        "        max_length=TARGET_MAX_TOKEN_LEN,\n",
        "        repetition_penalty=1.0,\n",
        "        length_penalty=1.0,\n",
        "        early_stopping=True,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    preds = {\n",
        "        tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        for generated_id in generated_ids\n",
        "    }\n",
        "\n",
        "    return ''.join(preds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1fa1JSwPRvi"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'multitask-qg-ag.ckpt'\n",
        "\n",
        "best_model = QGModel.load_from_checkpoint(checkpoint_path)\n",
        "best_model.freeze()\n",
        "best_model.eval()\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqQbJx1GO77G"
      },
      "outputs": [],
      "source": [
        "import nltk.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOfhToYeOk1t"
      },
      "outputs": [],
      "source": [
        "# the function that generate the questions and answer \n",
        "def question_generation(text):\n",
        "    tokenizerzz = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    tope = tokenizerzz.tokenize(text)\n",
        "    QandA = [] \n",
        "    for tee in tope:\n",
        "        generated = generate2(best_model, tee)\n",
        "        QandA.append(generated)\n",
        "    return QandA\n",
        "\n",
        "n = 1\n",
        "result = timeit.timeit(stmt='question_generation(text2)', globals=globals(), number=n)\n",
        "print(f\"Execution time is {result / n} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxlRYZmRxJPK"
      },
      "outputs": [],
      "source": [
        "# this function seperate the question and answer, checks if the question does not have answer, drop it from then list and print random 3 question \n",
        "\n",
        "import random\n",
        "def clean2(output):\n",
        "    \n",
        "    if type(output) == list:\n",
        "        result = []\n",
        "        for value in output:\n",
        "           \n",
        "            generated = generate2(best_model, value)\n",
        "            answer = generated.split(\"<sep>\")[0]\n",
        "            question = generated.split(\"<sep>\")[1]\n",
        "            if answer == \"\":\n",
        "                continue\n",
        "            dic_word = {\"question\" : question , \"answer\" : answer }\n",
        "            \n",
        "                     \n",
        "            result.append(json.dumps(dic_word, indent = 4))\n",
        "        print(random.choices(result, k = 3))    \n",
        "            \n",
        "\n",
        "    else:\n",
        "        answer = generated.split(\"<sep>\")[0]\n",
        "        question = generated.split(\"<sep>\")[1]\n",
        "        dic_word = {\"question\" : question , \"answer\" : answer }\n",
        "        output = json.dumps(dic_word, indent = 2) \n",
        "        print(output)\n",
        "\n",
        "n = 1\n",
        "result = timeit.timeit(stmt='clean2(QandA)', globals=globals(), number=n)\n",
        "print(f\"Execution time is {result / n} seconds\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "voicechat",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "c400be7b51d8b91e9d6318332ad3deb7bfaa26d7d01ca3968124d325b7fa373f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
